{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0_wx1b6icjRC",
        "R9EqlFSZ7KwI",
        "1ze4WZy6BXAm",
        "WO2VNtBAv3rw"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "347559df903540dd8ff69da3e85ec63b": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_79315cdbf90341ed97d2c17b8f0b7051",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[32m▰▰▰▰▱▱▱\u001b[0m Thinking...\n\u001b[36m┏━\u001b[0m\u001b[36m Message \u001b[0m\u001b[36m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[36m━┓\u001b[0m\n\u001b[36m┃\u001b[0m                                                                                                                 \u001b[36m┃\u001b[0m\n\u001b[36m┃\u001b[0m \u001b[32mShow me a list of title from top 5 most recent story from Hacker News\u001b[0m                                           \u001b[36m┃\u001b[0m\n\u001b[36m┃\u001b[0m                                                                                                                 \u001b[36m┃\u001b[0m\n\u001b[36m┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\u001b[0m\n\u001b[33m┏━\u001b[0m\u001b[33m Tool Calls \u001b[0m\u001b[33m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[33m━┓\u001b[0m\n\u001b[33m┃\u001b[0m                                                                                                                 \u001b[33m┃\u001b[0m\n\u001b[33m┃\u001b[0m • get_top_hackernews_stories(num_stories=5)                                                                     \u001b[33m┃\u001b[0m\n\u001b[33m┃\u001b[0m                                                                                                                 \u001b[33m┃\u001b[0m\n\u001b[33m┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\u001b[0m\n\u001b[34m┏━\u001b[0m\u001b[34m Response (7.3s) \u001b[0m\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[34m━┓\u001b[0m\n\u001b[34m┃\u001b[0m                                                                                                                 \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m Here are the top 5 most recent stories from Hacker News:                                                        \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m                                                                                                                 \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m 1 \u001b[0m\u001b[1mOpenAI adds MCP support to Agents SDK\u001b[0m                                                                        \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m   \u001b[0m\u001b]8;id=501521;https://openai.github.io/openai-agents-python/mcp/\u001b\\\u001b[4;34mRead more\u001b[0m\u001b]8;;\u001b\\                                                                                                    \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m 2 \u001b[0m\u001b[1mDebian bookworm live images now reproducible\u001b[0m                                                                 \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m   \u001b[0m\u001b]8;id=698136;https://lwn.net/Articles/1015402/\u001b\\\u001b[4;34mRead more\u001b[0m\u001b]8;;\u001b\\                                                                                                    \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m 3 \u001b[0m\u001b[1mThe Mysterious Flow of Fluid in the Brain\u001b[0m                                                                    \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m   \u001b[0m\u001b]8;id=599249;https://www.quantamagazine.org/the-mysterious-flow-of-fluid-in-the-brain-20250326/\u001b\\\u001b[4;34mRead more\u001b[0m\u001b]8;;\u001b\\                                                                                                    \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m 4 \u001b[0m\u001b[1mA love letter to the CSV format\u001b[0m                                                                              \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m   \u001b[0m\u001b]8;id=801033;https://github.com/medialab/xan/blob/master/docs/LOVE_LETTER.md\u001b\\\u001b[4;34mRead more\u001b[0m\u001b]8;;\u001b\\                                                                                                    \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m 5 \u001b[0m\u001b[1mBuilding a Linux Container Runtime from Scratch\u001b[0m                                                              \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m   \u001b[0m\u001b]8;id=180920;https://edera.dev/stories/styrolite\u001b\\\u001b[4;34mRead more\u001b[0m\u001b]8;;\u001b\\                                                                                                    \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m                                                                                                                 \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m Stay tuned for more tech tidbits! 🖥️🔍                                                                           \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m                                                                                                                 \u001b[34m┃\u001b[0m\n\u001b[34m┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">▰▰▰▰▱▱▱</span> Thinking...\n<span style=\"color: #008080; text-decoration-color: #008080\">┏━ Message ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓</span>\n<span style=\"color: #008080; text-decoration-color: #008080\">┃</span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\">┃</span>\n<span style=\"color: #008080; text-decoration-color: #008080\">┃</span> <span style=\"color: #008000; text-decoration-color: #008000\">Show me a list of title from top 5 most recent story from Hacker News</span>                                           <span style=\"color: #008080; text-decoration-color: #008080\">┃</span>\n<span style=\"color: #008080; text-decoration-color: #008080\">┃</span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\">┃</span>\n<span style=\"color: #008080; text-decoration-color: #008080\">┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛</span>\n<span style=\"color: #808000; text-decoration-color: #808000\">┏━ Tool Calls ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓</span>\n<span style=\"color: #808000; text-decoration-color: #808000\">┃</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">┃</span>\n<span style=\"color: #808000; text-decoration-color: #808000\">┃</span> • get_top_hackernews_stories(num_stories=5)                                                                     <span style=\"color: #808000; text-decoration-color: #808000\">┃</span>\n<span style=\"color: #808000; text-decoration-color: #808000\">┃</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">┃</span>\n<span style=\"color: #808000; text-decoration-color: #808000\">┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┏━ Response (7.3s) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> Here are the top 5 most recent stories from Hacker News:                                                        <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 1 </span><span style=\"font-weight: bold\">OpenAI adds MCP support to Agents SDK</span>                                                                        <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><a href=\"https://openai.github.io/openai-agents-python/mcp/\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080; text-decoration: underline\">Read more</span></a>                                                                                                    <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 2 </span><span style=\"font-weight: bold\">Debian bookworm live images now reproducible</span>                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><a href=\"https://lwn.net/Articles/1015402/\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080; text-decoration: underline\">Read more</span></a>                                                                                                    <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 3 </span><span style=\"font-weight: bold\">The Mysterious Flow of Fluid in the Brain</span>                                                                    <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><a href=\"https://www.quantamagazine.org/the-mysterious-flow-of-fluid-in-the-brain-20250326/\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080; text-decoration: underline\">Read more</span></a>                                                                                                    <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 4 </span><span style=\"font-weight: bold\">A love letter to the CSV format</span>                                                                              <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><a href=\"https://github.com/medialab/xan/blob/master/docs/LOVE_LETTER.md\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080; text-decoration: underline\">Read more</span></a>                                                                                                    <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 5 </span><span style=\"font-weight: bold\">Building a Linux Container Runtime from Scratch</span>                                                              <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><a href=\"https://edera.dev/stories/styrolite\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080; text-decoration: underline\">Read more</span></a>                                                                                                    <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> Stay tuned for more tech tidbits! 🖥️🔍                                                                           <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "79315cdbf90341ed97d2c17b8f0b7051": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Sinergia Dev** playbook"
      ],
      "metadata": {
        "id": "GPWEhGa0m2SU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Openai Responses API"
      ],
      "metadata": {
        "id": "7Rg0qWBno1O4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- python install requirements"
      ],
      "metadata": {
        "id": "aUiDqKuHo8xO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UYfa5CfsoicL",
        "outputId": "15c7ca6c-fae1-4c12-a5df-603a3d0706b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.68.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "# set env for openai key:\n",
        "\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")"
      ],
      "metadata": {
        "id": "kiDdYIHeb--6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "297b111a-31c7-4e0e-9ed6-c7f6599a55aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter API key for OpenAI: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "# Generate text from a simple prompt\n",
        "\n",
        "## ChatGPT LLM single call\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    input=\"Escribe un cuento de buenas noches en una sola oración sobre un unicornio.\"\n",
        ")\n",
        "\n",
        "## print response output text\n",
        "print(\"response.output_text:\\n\")\n",
        "print(response.output_text)\n",
        "\n",
        "## debug full response object\n",
        "print(\"response object:\\n\")\n",
        "pprint.pprint(response.model_dump())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "P9h03_IfpSGE",
        "outputId": "0b1c90be-371f-4987-c584-6fb6397196d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "En un bosque encantado bajo un cielo estrellado, un unicornio de brillante cuerno dorado danzaba entre las flores, llenando el aire de suaves susurros mágicos que arrullaban a todos los animales hasta que se quedaban dormidos, soñando con aventuras en mundos lejanos.\n",
            "response object:\n",
            "\n",
            "{'created_at': 1742693934.0,\n",
            " 'error': None,\n",
            " 'id': 'resp_67df662e9fe48192a9e673150b83a2c205de7011f3d2c940',\n",
            " 'incomplete_details': None,\n",
            " 'instructions': None,\n",
            " 'max_output_tokens': None,\n",
            " 'metadata': {},\n",
            " 'model': 'gpt-4o-mini-2024-07-18',\n",
            " 'object': 'response',\n",
            " 'output': [{'content': [{'annotations': [],\n",
            "                          'text': 'En un bosque encantado bajo un cielo '\n",
            "                                  'estrellado, un unicornio de brillante '\n",
            "                                  'cuerno dorado danzaba entre las flores, '\n",
            "                                  'llenando el aire de suaves susurros mágicos '\n",
            "                                  'que arrullaban a todos los animales hasta '\n",
            "                                  'que se quedaban dormidos, soñando con '\n",
            "                                  'aventuras en mundos lejanos.',\n",
            "                          'type': 'output_text'}],\n",
            "             'id': 'msg_67df662f10cc8192b4d988c73fd9a31105de7011f3d2c940',\n",
            "             'role': 'assistant',\n",
            "             'status': 'completed',\n",
            "             'type': 'message'}],\n",
            " 'parallel_tool_calls': True,\n",
            " 'previous_response_id': None,\n",
            " 'reasoning': {'effort': None, 'generate_summary': None},\n",
            " 'status': 'completed',\n",
            " 'store': True,\n",
            " 'temperature': 1.0,\n",
            " 'text': {'format': {'type': 'text'}},\n",
            " 'tool_choice': 'auto',\n",
            " 'tools': [],\n",
            " 'top_p': 1.0,\n",
            " 'truncation': 'disabled',\n",
            " 'usage': {'input_tokens': 41,\n",
            "           'input_tokens_details': {'cached_tokens': 0},\n",
            "           'output_tokens': 66,\n",
            "           'output_tokens_details': {'reasoning_tokens': 0},\n",
            "           'total_tokens': 107},\n",
            " 'user': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text with instructions\n",
        "\n",
        "## ChatGPT LLM single call\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4o\",\n",
        "    instructions=\"Your goals is to translate to english every user input\",\n",
        "    input=\"Escribe un cuento de buenas noches en una sola oración sobre un unicornio.\",\n",
        ")\n",
        "\n",
        "## print response output text\n",
        "print(\"response.output_text:\\n\")\n",
        "print(response.output_text)\n",
        "\n",
        "## debug full response object\n",
        "print(\"response object:\\n\")\n",
        "pprint.pprint(response.model_dump())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HvzCmUkerDzi",
        "outputId": "b9caa0ff-c31c-4d09-a26e-90d3d89c22d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response.output_text:\n",
            "\n",
            "Once upon a time, a unicorn with a shimmering rainbow mane gently guided the stars across the night sky to help lull the world to sleep.\n",
            "response object:\n",
            "\n",
            "{'created_at': 1742694376.0,\n",
            " 'error': None,\n",
            " 'id': 'resp_67df67e81f248192bc34a81eb67c65f80527f92665842bd4',\n",
            " 'incomplete_details': None,\n",
            " 'instructions': 'Your goals is to translate to english every user input',\n",
            " 'max_output_tokens': None,\n",
            " 'metadata': {},\n",
            " 'model': 'gpt-4o-2024-08-06',\n",
            " 'object': 'response',\n",
            " 'output': [{'content': [{'annotations': [],\n",
            "                          'text': 'Once upon a time, a unicorn with a '\n",
            "                                  'shimmering rainbow mane gently guided the '\n",
            "                                  'stars across the night sky to help lull the '\n",
            "                                  'world to sleep.',\n",
            "                          'type': 'output_text'}],\n",
            "             'id': 'msg_67df67e86fbc8192a59ebf9eb925f7410527f92665842bd4',\n",
            "             'role': 'assistant',\n",
            "             'status': 'completed',\n",
            "             'type': 'message'}],\n",
            " 'parallel_tool_calls': True,\n",
            " 'previous_response_id': None,\n",
            " 'reasoning': {'effort': None, 'generate_summary': None},\n",
            " 'status': 'completed',\n",
            " 'store': True,\n",
            " 'temperature': 1.0,\n",
            " 'text': {'format': {'type': 'text'}},\n",
            " 'tool_choice': 'auto',\n",
            " 'tools': [],\n",
            " 'top_p': 1.0,\n",
            " 'truncation': 'disabled',\n",
            " 'usage': {'input_tokens': 55,\n",
            "           'input_tokens_details': {'cached_tokens': 0},\n",
            "           'output_tokens': 29,\n",
            "           'output_tokens_details': {'reasoning_tokens': 0},\n",
            "           'total_tokens': 84},\n",
            " 'user': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text with messages using different roles\n",
        "\n",
        "## ChatGPT LLM with message history style (it will preserve instructions on history)\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    input=[\n",
        "        {\n",
        "            \"role\": \"developer\",\n",
        "            \"content\": \"Your goals is to translate to spanish every user input\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Are semicolons optional in JavaScript?\"\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "## print response output text\n",
        "print(\"response.output_text:\\n\")\n",
        "print(response.output_text)\n",
        "\n",
        "## debug full response object\n",
        "print(\"response object:\\n\")\n",
        "pprint.pprint(response.model_dump())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRIbvrqPxWJh",
        "outputId": "50cc73cb-ca46-4ee0-9354-0fa742b0f7cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response.output_text:\n",
            "\n",
            "¿Son opcionales los puntos y coma en JavaScript?\n",
            "response object:\n",
            "\n",
            "{'created_at': 1742696160.0,\n",
            " 'error': None,\n",
            " 'id': 'resp_67df6ee020e0819287dd0e3440d2334801e4bf2825cab03a',\n",
            " 'incomplete_details': None,\n",
            " 'instructions': None,\n",
            " 'max_output_tokens': None,\n",
            " 'metadata': {},\n",
            " 'model': 'gpt-4o-mini-2024-07-18',\n",
            " 'object': 'response',\n",
            " 'output': [{'content': [{'annotations': [],\n",
            "                          'text': '¿Son opcionales los puntos y coma en '\n",
            "                                  'JavaScript?',\n",
            "                          'type': 'output_text'}],\n",
            "             'id': 'msg_67df6ee092808192bf4409fa423b50fb01e4bf2825cab03a',\n",
            "             'role': 'assistant',\n",
            "             'status': 'completed',\n",
            "             'type': 'message'}],\n",
            " 'parallel_tool_calls': True,\n",
            " 'previous_response_id': None,\n",
            " 'reasoning': {'effort': None, 'generate_summary': None},\n",
            " 'status': 'completed',\n",
            " 'store': True,\n",
            " 'temperature': 1.0,\n",
            " 'text': {'format': {'type': 'text'}},\n",
            " 'tool_choice': 'auto',\n",
            " 'tools': [],\n",
            " 'top_p': 1.0,\n",
            " 'truncation': 'disabled',\n",
            " 'usage': {'input_tokens': 48,\n",
            "           'input_tokens_details': {'cached_tokens': 0},\n",
            "           'output_tokens': 13,\n",
            "           'output_tokens_details': {'reasoning_tokens': 0},\n",
            "           'total_tokens': 61},\n",
            " 'user': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Structured Outputs"
      ],
      "metadata": {
        "id": "jst96GmlyzTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# structured output models support by:\n",
        "\n",
        "# gpt-4.5-preview-2025-02-27 and later\n",
        "# o3-mini-2025-1-31 and later\n",
        "# o1-2024-12-17 and later\n",
        "# gpt-4o-mini-2024-07-18 and later\n",
        "# gpt-4o-2024-08-06 and later\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4o-2024-08-06\",\n",
        "    input=[\n",
        "        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Matias and Jony are going to SinergIA-Dev on 25 Friday, march at 15hs.\"}\n",
        "    ],\n",
        "    text={\n",
        "        \"format\": {\n",
        "            \"type\": \"json_schema\",\n",
        "            \"name\": \"calendar_event\",\n",
        "            \"schema\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"name\": {\n",
        "                        \"type\": \"string\"\n",
        "                    },\n",
        "                    \"date\": {\n",
        "                        \"type\": \"string\"\n",
        "                    },\n",
        "                    \"participants\": {\n",
        "                        \"type\": \"array\",\n",
        "                        \"items\": {\n",
        "                            \"type\": \"string\"\n",
        "                        }\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"name\", \"date\", \"participants\"],\n",
        "                \"additionalProperties\": False\n",
        "            },\n",
        "            \"strict\": True\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "event = json.loads(response.output_text)\n",
        "\n",
        "print(event)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiwAXtLSyxVs",
        "outputId": "c759f787-b0ff-422b-c021-72610b8cfb37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'SinergIA-Dev', 'date': '2023-03-25T15:00:00', 'participants': ['Matias', 'Jony']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pprint\n",
        "\n",
        "# Structured Outputs for chain-of-thought math tutoring\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4o-2024-08-06\",\n",
        "    input=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful math tutor. Guide the user through the solution step by step.\"},\n",
        "        {\"role\": \"user\", \"content\": \"how can I solve 8x + 7 = -23\"}\n",
        "    ],\n",
        "    text={\n",
        "        \"format\": {\n",
        "            \"type\": \"json_schema\",\n",
        "            \"name\": \"math_reasoning\",\n",
        "            \"schema\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"steps\": {\n",
        "                        \"type\": \"array\",\n",
        "                        \"items\": {\n",
        "                            \"type\": \"object\",\n",
        "                            \"properties\": {\n",
        "                                \"explanation\": { \"type\": \"string\" },\n",
        "                                \"output\": { \"type\": \"string\" }\n",
        "                            },\n",
        "                            \"required\": [\"explanation\", \"output\"],\n",
        "                            \"additionalProperties\": False\n",
        "                        }\n",
        "                    },\n",
        "                    \"final_answer\": { \"type\": \"string\" }\n",
        "                },\n",
        "                \"required\": [\"steps\", \"final_answer\"],\n",
        "                \"additionalProperties\": False\n",
        "            },\n",
        "            \"strict\": True\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "math_reasoning = json.loads(response.output_text)\n",
        "pprint.pprint(math_reasoning)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwKcmCPH0aqd",
        "outputId": "49294c06-48b6-4c52-a2fd-96aeb4782ba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'final_answer': 'x = -\\\\frac{15}{4}',\n",
            " 'steps': [{'explanation': \"Start by isolating the term with the variable 'x'. \"\n",
            "                           'We need to move the constant term on the left side '\n",
            "                           'to the right side. To do this, subtract 7 from '\n",
            "                           'both sides of the equation.',\n",
            "            'output': '8x + 7 - 7 = -23 - 7'},\n",
            "           {'explanation': 'This simplifies to:', 'output': '8x = -30'},\n",
            "           {'explanation': \"Next, solve for 'x' by dividing both sides of the \"\n",
            "                           'equation by 8.',\n",
            "            'output': '\\\\( x = \\\\frac{-30}{8} \\\\)'},\n",
            "           {'explanation': 'Simplify the fraction by dividing the numerator '\n",
            "                           'and the denominator by their greatest common '\n",
            "                           'divisor, which is 2.',\n",
            "            'output': '\\\\( x = \\\\frac{-15}{4} \\\\)'},\n",
            "           {'explanation': 'Thus, the solution to the equation is:',\n",
            "            'output': 'x = -\\\\frac{15}{4}'}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nYLXmhOCbYKy",
        "outputId": "dc976272-1500-4f14-bf56-c439b2c159b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.20)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.15)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.39)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# https://python.langchain.com/docs/concepts/prompt_templates/\n",
        "# install langchain\n",
        "\n",
        "!pip install langchain\n",
        "!pip install -qU \"langchain[openai]\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# define model to use\n",
        "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
        "\n",
        "# message preparation:\n",
        "\n",
        "## using history messages:\n",
        "messages = [\n",
        "    SystemMessage(\"Translate the following from English into Spanish\"),\n",
        "    HumanMessage(\"hi! Im a developer trying to learn AI\"),\n",
        "]\n",
        "\n",
        "# LLM call\n",
        "\n",
        "## invoke chat completion\n",
        "response = model.invoke(messages)\n",
        "\n",
        "# print response text\n",
        "print(\"response.content:\\n\")\n",
        "print(response.content + \"\\n\")\n",
        "print(\"response object:\\n\")\n",
        "pprint.pprint(response.model_dump())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6-Cj8aGbj-A",
        "outputId": "a87c3fa6-67c2-418c-e566-5d28768dbda5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response.content:\n",
            "\n",
            "¡Hola! Soy un desarrollador que está tratando de aprender sobre IA.\n",
            "\n",
            "response object:\n",
            "\n",
            "{'additional_kwargs': {'refusal': None},\n",
            " 'content': '¡Hola! Soy un desarrollador que está tratando de aprender sobre '\n",
            "            'IA.',\n",
            " 'example': False,\n",
            " 'id': 'run-6d050664-d16b-4c6b-827f-547a56ab17a3-0',\n",
            " 'invalid_tool_calls': [],\n",
            " 'name': None,\n",
            " 'response_metadata': {'finish_reason': 'stop',\n",
            "                       'id': 'chatcmpl-BE4TgDzmLvW32p12Wq7bFpTJMRnSu',\n",
            "                       'logprobs': None,\n",
            "                       'model_name': 'gpt-4o-mini-2024-07-18',\n",
            "                       'system_fingerprint': 'fp_b8bc95a0ac',\n",
            "                       'token_usage': {'completion_tokens': 16,\n",
            "                                       'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
            "                                                                     'audio_tokens': 0,\n",
            "                                                                     'reasoning_tokens': 0,\n",
            "                                                                     'rejected_prediction_tokens': 0},\n",
            "                                       'prompt_tokens': 27,\n",
            "                                       'prompt_tokens_details': {'audio_tokens': 0,\n",
            "                                                                 'cached_tokens': 0},\n",
            "                                       'total_tokens': 43}},\n",
            " 'tool_calls': [],\n",
            " 'type': 'ai',\n",
            " 'usage_metadata': {'input_token_details': {'audio': 0, 'cache_read': 0},\n",
            "                    'input_tokens': 27,\n",
            "                    'output_token_details': {'audio': 0, 'reasoning': 0},\n",
            "                    'output_tokens': 16,\n",
            "                    'total_tokens': 43}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example for a dynamic template completion\n",
        "\n",
        "## using template with variables\n",
        "system_template = \"Translate the following from English into {language}\" # variable on template interpolated with {var_name}\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
        ")\n",
        "\n",
        "prompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi! Please take the recommended actions to update your configuration\"})\n",
        "\n"
      ],
      "metadata": {
        "id": "hakqOW1PjREl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template_prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
        "\n",
        "chain = template_prompt | model\n",
        "response1 = chain.invoke({\"topic\": \"programming\"})\n",
        "print(response1.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6TUcu9DcXnD",
        "outputId": "ff9d7c3a-d09b-46fa-88f2-0bac1f0d22c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why do programmers prefer dark mode?  \n",
            "\n",
            "Because light attracts bugs!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "\n",
        "pprint.pprint(response1.model_dump())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gtIdQ1hfQ6b",
        "outputId": "91712a1c-60eb-43c9-ee29-0040cf84cd11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'additional_kwargs': {'refusal': None},\n",
            " 'content': 'Why do programmers prefer dark mode?  \\n'\n",
            "            '\\n'\n",
            "            'Because light attracts bugs!',\n",
            " 'example': False,\n",
            " 'id': 'run-f082ff71-da76-4769-bdbe-6d1b237dd666-0',\n",
            " 'invalid_tool_calls': [],\n",
            " 'name': None,\n",
            " 'response_metadata': {'finish_reason': 'stop',\n",
            "                       'id': 'chatcmpl-BE3qElWGd9rZe1lS6y28RiISSkMhz',\n",
            "                       'logprobs': None,\n",
            "                       'model_name': 'gpt-4o-mini-2024-07-18',\n",
            "                       'system_fingerprint': 'fp_b8bc95a0ac',\n",
            "                       'token_usage': {'completion_tokens': 14,\n",
            "                                       'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
            "                                                                     'audio_tokens': 0,\n",
            "                                                                     'reasoning_tokens': 0,\n",
            "                                                                     'rejected_prediction_tokens': 0},\n",
            "                                       'prompt_tokens': 13,\n",
            "                                       'prompt_tokens_details': {'audio_tokens': 0,\n",
            "                                                                 'cached_tokens': 0},\n",
            "                                       'total_tokens': 27}},\n",
            " 'tool_calls': [],\n",
            " 'type': 'ai',\n",
            " 'usage_metadata': {'input_token_details': {'audio': 0, 'cache_read': 0},\n",
            "                    'input_tokens': 13,\n",
            "                    'output_token_details': {'audio': 0, 'reasoning': 0},\n",
            "                    'output_tokens': 14,\n",
            "                    'total_tokens': 27}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4jLb43RLfqq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sinergia Dev** playbook"
      ],
      "metadata": {
        "id": "CpUc3rMFcjQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Openai Responses API"
      ],
      "metadata": {
        "id": "quOWiBsgcjRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- python install requirements"
      ],
      "metadata": {
        "id": "53KD6q_ycjRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "a9d02f09-1dd9-42e0-f612-e40d65b499ab",
        "id": "bJLsSTJxcjRA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.66.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "# set env for openai key:\n",
        "\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")"
      ],
      "metadata": {
        "id": "P1jha6itcjRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RpFsKCmD6rWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompting OAi responses"
      ],
      "metadata": {
        "id": "gT6zSWR16sjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://platform.openai.com/docs/guides/text?api-mode=responses\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "# Generate text from a simple prompt\n",
        "\n",
        "## ChatGPT LLM single call\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    input=\"Escribe un cuento de buenas noches en una sola oración sobre un unicornio.\"\n",
        ")\n",
        "\n",
        "## print response output text\n",
        "print(\"response.output_text:\\n\")\n",
        "print(response.output_text)\n",
        "\n",
        "## debug full response object\n",
        "print(\"response object:\\n\")\n",
        "pprint.pprint(response.model_dump())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "0b1c90be-371f-4987-c584-6fb6397196d4",
        "id": "IskA7o0PcjRB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "En un bosque encantado bajo un cielo estrellado, un unicornio de brillante cuerno dorado danzaba entre las flores, llenando el aire de suaves susurros mágicos que arrullaban a todos los animales hasta que se quedaban dormidos, soñando con aventuras en mundos lejanos.\n",
            "response object:\n",
            "\n",
            "{'created_at': 1742693934.0,\n",
            " 'error': None,\n",
            " 'id': 'resp_67df662e9fe48192a9e673150b83a2c205de7011f3d2c940',\n",
            " 'incomplete_details': None,\n",
            " 'instructions': None,\n",
            " 'max_output_tokens': None,\n",
            " 'metadata': {},\n",
            " 'model': 'gpt-4o-mini-2024-07-18',\n",
            " 'object': 'response',\n",
            " 'output': [{'content': [{'annotations': [],\n",
            "                          'text': 'En un bosque encantado bajo un cielo '\n",
            "                                  'estrellado, un unicornio de brillante '\n",
            "                                  'cuerno dorado danzaba entre las flores, '\n",
            "                                  'llenando el aire de suaves susurros mágicos '\n",
            "                                  'que arrullaban a todos los animales hasta '\n",
            "                                  'que se quedaban dormidos, soñando con '\n",
            "                                  'aventuras en mundos lejanos.',\n",
            "                          'type': 'output_text'}],\n",
            "             'id': 'msg_67df662f10cc8192b4d988c73fd9a31105de7011f3d2c940',\n",
            "             'role': 'assistant',\n",
            "             'status': 'completed',\n",
            "             'type': 'message'}],\n",
            " 'parallel_tool_calls': True,\n",
            " 'previous_response_id': None,\n",
            " 'reasoning': {'effort': None, 'generate_summary': None},\n",
            " 'status': 'completed',\n",
            " 'store': True,\n",
            " 'temperature': 1.0,\n",
            " 'text': {'format': {'type': 'text'}},\n",
            " 'tool_choice': 'auto',\n",
            " 'tools': [],\n",
            " 'top_p': 1.0,\n",
            " 'truncation': 'disabled',\n",
            " 'usage': {'input_tokens': 41,\n",
            "           'input_tokens_details': {'cached_tokens': 0},\n",
            "           'output_tokens': 66,\n",
            "           'output_tokens_details': {'reasoning_tokens': 0},\n",
            "           'total_tokens': 107},\n",
            " 'user': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text with instructions\n",
        "\n",
        "## ChatGPT LLM single call\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4o\",\n",
        "    instructions=\"Your goals is to translate to english every user input\",\n",
        "    input=\"Escribe un cuento de buenas noches en una sola oración sobre un unicornio.\",\n",
        ")\n",
        "\n",
        "## print response output text\n",
        "print(\"response.output_text:\\n\")\n",
        "print(response.output_text)\n",
        "\n",
        "## debug full response object\n",
        "print(\"response object:\\n\")\n",
        "pprint.pprint(response.model_dump())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "b9caa0ff-c31c-4d09-a26e-90d3d89c22d5",
        "id": "LMBtLMC0cjRC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response.output_text:\n",
            "\n",
            "Once upon a time, a unicorn with a shimmering rainbow mane gently guided the stars across the night sky to help lull the world to sleep.\n",
            "response object:\n",
            "\n",
            "{'created_at': 1742694376.0,\n",
            " 'error': None,\n",
            " 'id': 'resp_67df67e81f248192bc34a81eb67c65f80527f92665842bd4',\n",
            " 'incomplete_details': None,\n",
            " 'instructions': 'Your goals is to translate to english every user input',\n",
            " 'max_output_tokens': None,\n",
            " 'metadata': {},\n",
            " 'model': 'gpt-4o-2024-08-06',\n",
            " 'object': 'response',\n",
            " 'output': [{'content': [{'annotations': [],\n",
            "                          'text': 'Once upon a time, a unicorn with a '\n",
            "                                  'shimmering rainbow mane gently guided the '\n",
            "                                  'stars across the night sky to help lull the '\n",
            "                                  'world to sleep.',\n",
            "                          'type': 'output_text'}],\n",
            "             'id': 'msg_67df67e86fbc8192a59ebf9eb925f7410527f92665842bd4',\n",
            "             'role': 'assistant',\n",
            "             'status': 'completed',\n",
            "             'type': 'message'}],\n",
            " 'parallel_tool_calls': True,\n",
            " 'previous_response_id': None,\n",
            " 'reasoning': {'effort': None, 'generate_summary': None},\n",
            " 'status': 'completed',\n",
            " 'store': True,\n",
            " 'temperature': 1.0,\n",
            " 'text': {'format': {'type': 'text'}},\n",
            " 'tool_choice': 'auto',\n",
            " 'tools': [],\n",
            " 'top_p': 1.0,\n",
            " 'truncation': 'disabled',\n",
            " 'usage': {'input_tokens': 55,\n",
            "           'input_tokens_details': {'cached_tokens': 0},\n",
            "           'output_tokens': 29,\n",
            "           'output_tokens_details': {'reasoning_tokens': 0},\n",
            "           'total_tokens': 84},\n",
            " 'user': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text with messages using different roles\n",
        "\n",
        "## ChatGPT LLM with message history style (it will preserve instructions on history)\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    input=[\n",
        "        {\n",
        "            \"role\": \"developer\",\n",
        "            \"content\": \"Your goals is to translate to spanish every user input\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Are semicolons optional in JavaScript?\"\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "## print response output text\n",
        "print(\"response.output_text:\\n\")\n",
        "print(response.output_text)\n",
        "\n",
        "## debug full response object\n",
        "print(\"response object:\\n\")\n",
        "pprint.pprint(response.model_dump())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50cc73cb-ca46-4ee0-9354-0fa742b0f7cd",
        "id": "SWi_p5g2cjRC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response.output_text:\n",
            "\n",
            "¿Son opcionales los puntos y coma en JavaScript?\n",
            "response object:\n",
            "\n",
            "{'created_at': 1742696160.0,\n",
            " 'error': None,\n",
            " 'id': 'resp_67df6ee020e0819287dd0e3440d2334801e4bf2825cab03a',\n",
            " 'incomplete_details': None,\n",
            " 'instructions': None,\n",
            " 'max_output_tokens': None,\n",
            " 'metadata': {},\n",
            " 'model': 'gpt-4o-mini-2024-07-18',\n",
            " 'object': 'response',\n",
            " 'output': [{'content': [{'annotations': [],\n",
            "                          'text': '¿Son opcionales los puntos y coma en '\n",
            "                                  'JavaScript?',\n",
            "                          'type': 'output_text'}],\n",
            "             'id': 'msg_67df6ee092808192bf4409fa423b50fb01e4bf2825cab03a',\n",
            "             'role': 'assistant',\n",
            "             'status': 'completed',\n",
            "             'type': 'message'}],\n",
            " 'parallel_tool_calls': True,\n",
            " 'previous_response_id': None,\n",
            " 'reasoning': {'effort': None, 'generate_summary': None},\n",
            " 'status': 'completed',\n",
            " 'store': True,\n",
            " 'temperature': 1.0,\n",
            " 'text': {'format': {'type': 'text'}},\n",
            " 'tool_choice': 'auto',\n",
            " 'tools': [],\n",
            " 'top_p': 1.0,\n",
            " 'truncation': 'disabled',\n",
            " 'usage': {'input_tokens': 48,\n",
            "           'input_tokens_details': {'cached_tokens': 0},\n",
            "           'output_tokens': 13,\n",
            "           'output_tokens_details': {'reasoning_tokens': 0},\n",
            "           'total_tokens': 61},\n",
            " 'user': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Structured Outputs"
      ],
      "metadata": {
        "id": "0_wx1b6icjRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses\n",
        "import json\n",
        "\n",
        "# structured output models support by:\n",
        "\n",
        "# gpt-4.5-preview-2025-02-27 and later\n",
        "# o3-mini-2025-1-31 and later\n",
        "# o1-2024-12-17 and later\n",
        "# gpt-4o-mini-2024-07-18 and later\n",
        "# gpt-4o-2024-08-06 and later\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4o-2024-08-06\",\n",
        "    input=[\n",
        "        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Matias and Jony are going to SinergIA-Dev on 25 Friday, march at 15hs.\"}\n",
        "    ],\n",
        "    text={\n",
        "        \"format\": {\n",
        "            \"type\": \"json_schema\",\n",
        "            \"name\": \"calendar_event\",\n",
        "            \"schema\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"name\": {\n",
        "                        \"type\": \"string\"\n",
        "                    },\n",
        "                    \"date\": {\n",
        "                        \"type\": \"string\"\n",
        "                    },\n",
        "                    \"participants\": {\n",
        "                        \"type\": \"array\",\n",
        "                        \"items\": {\n",
        "                            \"type\": \"string\"\n",
        "                        }\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"name\", \"date\", \"participants\"],\n",
        "                \"additionalProperties\": False\n",
        "            },\n",
        "            \"strict\": True\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "event = json.loads(response.output_text)\n",
        "\n",
        "print(event)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c759f787-b0ff-422b-c021-72610b8cfb37",
        "id": "y9ktzbNhcjRC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'SinergIA-Dev', 'date': '2023-03-25T15:00:00', 'participants': ['Matias', 'Jony']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pprint\n",
        "\n",
        "# Structured Outputs for chain-of-thought math tutoring\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4o-2024-08-06\",\n",
        "    input=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful math tutor. Guide the user through the solution step by step.\"},\n",
        "        {\"role\": \"user\", \"content\": \"how can I solve 8x + 7 = -23\"}\n",
        "    ],\n",
        "    text={\n",
        "        \"format\": {\n",
        "            \"type\": \"json_schema\",\n",
        "            \"name\": \"math_reasoning\",\n",
        "            \"schema\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"steps\": {\n",
        "                        \"type\": \"array\",\n",
        "                        \"items\": {\n",
        "                            \"type\": \"object\",\n",
        "                            \"properties\": {\n",
        "                                \"explanation\": { \"type\": \"string\" },\n",
        "                                \"output\": { \"type\": \"string\" }\n",
        "                            },\n",
        "                            \"required\": [\"explanation\", \"output\"],\n",
        "                            \"additionalProperties\": False\n",
        "                        }\n",
        "                    },\n",
        "                    \"final_answer\": { \"type\": \"string\" }\n",
        "                },\n",
        "                \"required\": [\"steps\", \"final_answer\"],\n",
        "                \"additionalProperties\": False\n",
        "            },\n",
        "            \"strict\": True\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "math_reasoning = json.loads(response.output_text)\n",
        "pprint.pprint(math_reasoning)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49294c06-48b6-4c52-a2fd-96aeb4782ba0",
        "id": "Nnuf4k8mcjRC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'final_answer': 'x = -\\\\frac{15}{4}',\n",
            " 'steps': [{'explanation': \"Start by isolating the term with the variable 'x'. \"\n",
            "                           'We need to move the constant term on the left side '\n",
            "                           'to the right side. To do this, subtract 7 from '\n",
            "                           'both sides of the equation.',\n",
            "            'output': '8x + 7 - 7 = -23 - 7'},\n",
            "           {'explanation': 'This simplifies to:', 'output': '8x = -30'},\n",
            "           {'explanation': \"Next, solve for 'x' by dividing both sides of the \"\n",
            "                           'equation by 8.',\n",
            "            'output': '\\\\( x = \\\\frac{-30}{8} \\\\)'},\n",
            "           {'explanation': 'Simplify the fraction by dividing the numerator '\n",
            "                           'and the denominator by their greatest common '\n",
            "                           'divisor, which is 2.',\n",
            "            'output': '\\\\( x = \\\\frac{-15}{4} \\\\)'},\n",
            "           {'explanation': 'Thus, the solution to the equation is:',\n",
            "            'output': 'x = -\\\\frac{15}{4}'}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "dc976272-1500-4f14-bf56-c439b2c159b6",
        "id": "S10THGtscjRC"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.20)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.15)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.39)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# https://python.langchain.com/docs/tutorials/llm_chain/\n",
        "# install langchain\n",
        "\n",
        "!pip install langchain\n",
        "!pip install -qU \"langchain[openai]\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# define model to use\n",
        "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
        "\n",
        "# message preparation:\n",
        "\n",
        "## using history messages:\n",
        "messages = [\n",
        "    SystemMessage(\"Translate the following from English into Spanish\"),\n",
        "    HumanMessage(\"hi! Im a developer trying to learn AI\"),\n",
        "]\n",
        "\n",
        "# LLM call\n",
        "\n",
        "## invoke chat completion\n",
        "response = model.invoke(messages)\n",
        "\n",
        "# print response text\n",
        "print(\"response.content:\\n\")\n",
        "print(response.content + \"\\n\")\n",
        "print(\"response object:\\n\")\n",
        "pprint.pprint(response.model_dump())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a87c3fa6-67c2-418c-e566-5d28768dbda5",
        "id": "uWNd1FbFcjRD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response.content:\n",
            "\n",
            "¡Hola! Soy un desarrollador que está tratando de aprender sobre IA.\n",
            "\n",
            "response object:\n",
            "\n",
            "{'additional_kwargs': {'refusal': None},\n",
            " 'content': '¡Hola! Soy un desarrollador que está tratando de aprender sobre '\n",
            "            'IA.',\n",
            " 'example': False,\n",
            " 'id': 'run-6d050664-d16b-4c6b-827f-547a56ab17a3-0',\n",
            " 'invalid_tool_calls': [],\n",
            " 'name': None,\n",
            " 'response_metadata': {'finish_reason': 'stop',\n",
            "                       'id': 'chatcmpl-BE4TgDzmLvW32p12Wq7bFpTJMRnSu',\n",
            "                       'logprobs': None,\n",
            "                       'model_name': 'gpt-4o-mini-2024-07-18',\n",
            "                       'system_fingerprint': 'fp_b8bc95a0ac',\n",
            "                       'token_usage': {'completion_tokens': 16,\n",
            "                                       'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
            "                                                                     'audio_tokens': 0,\n",
            "                                                                     'reasoning_tokens': 0,\n",
            "                                                                     'rejected_prediction_tokens': 0},\n",
            "                                       'prompt_tokens': 27,\n",
            "                                       'prompt_tokens_details': {'audio_tokens': 0,\n",
            "                                                                 'cached_tokens': 0},\n",
            "                                       'total_tokens': 43}},\n",
            " 'tool_calls': [],\n",
            " 'type': 'ai',\n",
            " 'usage_metadata': {'input_token_details': {'audio': 0, 'cache_read': 0},\n",
            "                    'input_tokens': 27,\n",
            "                    'output_token_details': {'audio': 0, 'reasoning': 0},\n",
            "                    'output_tokens': 16,\n",
            "                    'total_tokens': 43}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example for a dynamic template completion\n",
        "\n",
        "## using template with variables\n",
        "system_template = \"Translate the following from English into {language}\" # variable on template interpolated with {var_name}\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
        ")\n",
        "\n",
        "prompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi! Please take the recommended actions to update your configuration\"})\n",
        "\n"
      ],
      "metadata": {
        "id": "FT67Xxe8cjRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template_prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
        "\n",
        "chain = template_prompt | model\n",
        "response1 = chain.invoke({\"topic\": \"programming\"})\n",
        "print(response1.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff9d7c3a-d09b-46fa-88f2-0bac1f0d22c3",
        "id": "1XrdDIF8cjRD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why do programmers prefer dark mode?  \n",
            "\n",
            "Because light attracts bugs!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "\n",
        "pprint.pprint(response1.model_dump())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91712a1c-60eb-43c9-ee29-0040cf84cd11",
        "id": "5QMSNwZFcjRD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'additional_kwargs': {'refusal': None},\n",
            " 'content': 'Why do programmers prefer dark mode?  \\n'\n",
            "            '\\n'\n",
            "            'Because light attracts bugs!',\n",
            " 'example': False,\n",
            " 'id': 'run-f082ff71-da76-4769-bdbe-6d1b237dd666-0',\n",
            " 'invalid_tool_calls': [],\n",
            " 'name': None,\n",
            " 'response_metadata': {'finish_reason': 'stop',\n",
            "                       'id': 'chatcmpl-BE3qElWGd9rZe1lS6y28RiISSkMhz',\n",
            "                       'logprobs': None,\n",
            "                       'model_name': 'gpt-4o-mini-2024-07-18',\n",
            "                       'system_fingerprint': 'fp_b8bc95a0ac',\n",
            "                       'token_usage': {'completion_tokens': 14,\n",
            "                                       'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
            "                                                                     'audio_tokens': 0,\n",
            "                                                                     'reasoning_tokens': 0,\n",
            "                                                                     'rejected_prediction_tokens': 0},\n",
            "                                       'prompt_tokens': 13,\n",
            "                                       'prompt_tokens_details': {'audio_tokens': 0,\n",
            "                                                                 'cached_tokens': 0},\n",
            "                                       'total_tokens': 27}},\n",
            " 'tool_calls': [],\n",
            " 'type': 'ai',\n",
            " 'usage_metadata': {'input_token_details': {'audio': 0, 'cache_read': 0},\n",
            "                    'input_tokens': 13,\n",
            "                    'output_token_details': {'audio': 0, 'reasoning': 0},\n",
            "                    'output_tokens': 14,\n",
            "                    'total_tokens': 27}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aDSzL-XDcjRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Composed LLM (chaining)"
      ],
      "metadata": {
        "id": "R9EqlFSZ7KwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- pedir un boceto de algo al llm\n",
        "- pedir una critica del boceto\n",
        "- pedir algo al llm incluyendo el boceto y la critica."
      ],
      "metadata": {
        "id": "KzkqXIv_7Wnb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-iRx5SeK7Zju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agents: Human in the loop"
      ],
      "metadata": {
        "id": "1ze4WZy6BXAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/agno-agi/agno"
      ],
      "metadata": {
        "id": "eSGMBhKv58lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install httpx rich agno"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eMRAe3Rt7bU2",
        "outputId": "fc584537-d8f0-4d17-88f0-012e6edecc3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (0.28.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (13.9.4)\n",
            "Collecting agno\n",
            "  Downloading agno-1.2.4-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx) (0.14.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich) (2.18.0)\n",
            "Requirement already satisfied: docstring-parser in /usr/local/lib/python3.11/dist-packages (from agno) (0.16)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.11/dist-packages (from agno) (3.1.44)\n",
            "Collecting pydantic-settings (from agno)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from agno) (2.10.6)\n",
            "Collecting python-dotenv (from agno)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting python-multipart (from agno)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from agno) (6.0.2)\n",
            "Collecting tomli (from agno)\n",
            "  Downloading tomli-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.11/dist-packages (from agno) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from agno) (4.12.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx) (1.3.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython->agno) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->agno) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->agno) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer->agno) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer->agno) (1.5.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython->agno) (5.0.2)\n",
            "Downloading agno-1.2.4-py3-none-any.whl (574 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m574.9/574.9 kB\u001b[0m \u001b[31m822.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading tomli-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (236 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.0/236.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tomli, python-multipart, python-dotenv, pydantic-settings, agno\n",
            "Successfully installed agno-1.2.4 pydantic-settings-2.8.1 python-dotenv-1.1.0 python-multipart-0.0.20 tomli-2.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"🤝 Human-in-the-Loop: Adding User Confirmation to Tool Calls\n",
        "\n",
        "This example shows how to implement human-in-the-loop functionality in your Agno tools.\n",
        "It shows how to:\n",
        "- Add pre-hooks to tools for user confirmation\n",
        "- Handle user input during tool execution\n",
        "- Gracefully cancel operations based on user choice\n",
        "\n",
        "Some practical applications:\n",
        "- Confirming sensitive operations before execution\n",
        "- Reviewing API calls before they're made\n",
        "- Validating data transformations\n",
        "- Approving automated actions in critical systems\n",
        "\n",
        "Run `pip install openai httpx rich agno` to install dependencies.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "from textwrap import dedent\n",
        "from typing import Iterator\n",
        "\n",
        "import httpx\n",
        "from agno.agent import Agent\n",
        "from agno.exceptions import StopAgentRun\n",
        "from agno.tools import FunctionCall, tool\n",
        "from rich.console import Console\n",
        "from rich.pretty import pprint\n",
        "from rich.prompt import Prompt\n",
        "\n",
        "# This is the console instance used by the print_response method\n",
        "# We can use this to stop and restart the live display and ask for user confirmation\n",
        "console = Console()\n",
        "\n",
        "\n",
        "def pre_hook(fc: FunctionCall):\n",
        "    # Get the live display instance from the console\n",
        "    live = console._live\n",
        "\n",
        "    # Stop the live display temporarily so we can ask for user confirmation\n",
        "    live.stop()  # type: ignore\n",
        "\n",
        "    # Ask for confirmation\n",
        "    console.print(f\"\\nAbout to run [bold blue]{fc.function.name}[/]\")\n",
        "    message = (\n",
        "        Prompt.ask(\"Do you want to continue?\", choices=[\"y\", \"n\"], default=\"y\")\n",
        "        .strip()\n",
        "        .lower()\n",
        "    )\n",
        "\n",
        "    # Restart the live display\n",
        "    live.start()  # type: ignore\n",
        "\n",
        "    # If the user does not want to continue, raise a StopExecution exception\n",
        "    if message != \"y\":\n",
        "        raise StopAgentRun(\n",
        "            \"Tool call cancelled by user\",\n",
        "            agent_message=\"Stopping execution as permission was not granted.\",\n",
        "        )\n",
        "\n",
        "\n",
        "@tool(pre_hook=pre_hook)\n",
        "def get_top_hackernews_stories(num_stories: int) -> Iterator[str]:\n",
        "    \"\"\"Fetch top stories from Hacker News after user confirmation.\n",
        "\n",
        "    Args:\n",
        "        num_stories (int): Number of stories to retrieve\n",
        "\n",
        "    Returns:\n",
        "        str: JSON string containing story details\n",
        "    \"\"\"\n",
        "    # Fetch top story IDs\n",
        "    response = httpx.get(\"https://hacker-news.firebaseio.com/v0/topstories.json\")\n",
        "    story_ids = response.json()\n",
        "\n",
        "    # Yield story details\n",
        "    for story_id in story_ids[:num_stories]:\n",
        "        story_response = httpx.get(\n",
        "            f\"https://hacker-news.firebaseio.com/v0/item/{story_id}.json\"\n",
        "        )\n",
        "        story = story_response.json()\n",
        "        if \"text\" in story:\n",
        "            story.pop(\"text\", None)\n",
        "        yield json.dumps(story)\n",
        "\n",
        "\n",
        "# Initialize the agent with a tech-savvy personality and clear instructions\n",
        "agent = Agent(\n",
        "    description=\"A Tech News Assistant that fetches and summarizes Hacker News stories\",\n",
        "    instructions=dedent(\"\"\"\\\n",
        "        You are an enthusiastic Tech Reporter\n",
        "\n",
        "        Your responsibilities:\n",
        "        - Present Hacker News stories in an engaging and informative way\n",
        "        - Provide clear summaries of the information you gather\n",
        "\n",
        "        Style guide:\n",
        "        - Use emoji to make your responses more engaging\n",
        "        - Keep your summaries concise but informative\n",
        "        - End with a friendly tech-themed sign-off\\\n",
        "    \"\"\"),\n",
        "    tools=[get_top_hackernews_stories],\n",
        "    show_tool_calls=True,\n",
        "    markdown=True,\n",
        ")\n",
        "\n",
        "# Example questions to try:\n",
        "# - \"What are the top 3 HN stories right now?\"\n",
        "# - \"Show me the most recent story from Hacker News\"\n",
        "# - \"Get the top 5 stories (you can try accepting and declining the confirmation)\"\n",
        "agent.print_response(\n",
        "    \"Show me a list of title from top 5 most recent story from Hacker News\", stream=True, console=console\n",
        ")\n",
        "\n",
        "# View all messages\n",
        "# pprint(agent.run_response.messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537,
          "referenced_widgets": [
            "347559df903540dd8ff69da3e85ec63b",
            "79315cdbf90341ed97d2c17b8f0b7051"
          ]
        },
        "collapsed": true,
        "id": "W6lpRf05AiX7",
        "outputId": "48bfd5e0-1bb0-42e2-e3c7-cbaee7190e0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "347559df903540dd8ff69da3e85ec63b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "About to run \u001b[1;34mget_top_hackernews_stories\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "About to run <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">get_top_hackernews_stories</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Do you want to continue? \u001b[1;35m[y/n]\u001b[0m \u001b[1;36m(y)\u001b[0m: "
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Do you want to continue? <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">[y/n]</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(y)</span>: </pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "EH3MRILz7OBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memory w/mem0"
      ],
      "metadata": {
        "id": "WO2VNtBAv3rw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/mem0ai/mem0"
      ],
      "metadata": {
        "id": "UwBuIBKh5y3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jN9IF8aR7gMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mem0ai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QhEgL0oov_rM",
        "outputId": "a2c31c7a-510c-49e6-a8fd-258c1ac4f2a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mem0ai\n",
            "  Downloading mem0ai-0.1.77-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting azure-search-documents<12.0.0,>=11.5.0 (from mem0ai)\n",
            "  Downloading azure_search_documents-11.5.2-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.33.0 in /usr/local/lib/python3.11/dist-packages (from mem0ai) (1.68.2)\n",
            "Collecting posthog<4.0.0,>=3.5.0 (from mem0ai)\n",
            "  Downloading posthog-3.23.0-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting psycopg2-binary<3.0.0,>=2.9.10 (from mem0ai)\n",
            "  Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.3 in /usr/local/lib/python3.11/dist-packages (from mem0ai) (2.10.6)\n",
            "Collecting pytz<2025.0,>=2024.1 (from mem0ai)\n",
            "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting qdrant-client<2.0.0,>=1.9.1 (from mem0ai)\n",
            "  Downloading qdrant_client-1.13.3-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: sqlalchemy<3.0.0,>=2.0.31 in /usr/local/lib/python3.11/dist-packages (from mem0ai) (2.0.39)\n",
            "Collecting azure-core>=1.28.0 (from azure-search-documents<12.0.0,>=11.5.0->mem0ai)\n",
            "  Downloading azure_core-1.32.0-py3-none-any.whl.metadata (39 kB)\n",
            "Collecting azure-common>=1.1 (from azure-search-documents<12.0.0,>=11.5.0->mem0ai)\n",
            "  Downloading azure_common-1.1.28-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting isodate>=0.6.0 (from azure-search-documents<12.0.0,>=11.5.0->mem0ai)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from azure-search-documents<12.0.0,>=11.5.0->mem0ai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.33.0->mem0ai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.33.0->mem0ai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.33.0->mem0ai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.33.0->mem0ai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.33.0->mem0ai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.33.0->mem0ai) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.7 in /usr/local/lib/python3.11/dist-packages (from posthog<4.0.0,>=3.5.0->mem0ai) (2.32.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from posthog<4.0.0,>=3.5.0->mem0ai) (1.17.0)\n",
            "Collecting monotonic>=1.5 (from posthog<4.0.0,>=3.5.0->mem0ai)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<4.0.0,>=3.5.0->mem0ai)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.11/dist-packages (from posthog<4.0.0,>=3.5.0->mem0ai) (2.8.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.3->mem0ai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.3->mem0ai) (2.27.2)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.11/dist-packages (from qdrant-client<2.0.0,>=1.9.1->mem0ai) (1.71.0)\n",
            "Collecting grpcio-tools>=1.41.0 (from qdrant-client<2.0.0,>=1.9.1->mem0ai)\n",
            "  Downloading grpcio_tools-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.11/dist-packages (from qdrant-client<2.0.0,>=1.9.1->mem0ai) (2.0.2)\n",
            "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant-client<2.0.0,>=1.9.1->mem0ai)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.11/dist-packages (from qdrant-client<2.0.0,>=1.9.1->mem0ai) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3.0.0,>=2.0.31->mem0ai) (3.1.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.33.0->mem0ai) (3.10)\n",
            "Requirement already satisfied: protobuf<6.0dev,>=5.26.1 in /usr/local/lib/python3.11/dist-packages (from grpcio-tools>=1.41.0->qdrant-client<2.0.0,>=1.9.1->mem0ai) (5.29.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from grpcio-tools>=1.41.0->qdrant-client<2.0.0,>=1.9.1->mem0ai) (75.1.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.33.0->mem0ai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.33.0->mem0ai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.33.0->mem0ai) (0.14.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai) (4.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.7->posthog<4.0.0,>=3.5.0->mem0ai) (3.4.1)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai) (4.1.0)\n",
            "Downloading mem0ai-0.1.77-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_search_documents-11.5.2-py3-none-any.whl (298 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.8/298.8 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-3.23.0-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qdrant_client-1.13.3-py3-none-any.whl (306 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.7/306.7 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
            "Downloading azure_core-1.32.0-py3-none-any.whl (198 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.9/198.9 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading grpcio_tools-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: pytz, monotonic, azure-common, psycopg2-binary, portalocker, isodate, grpcio-tools, backoff, posthog, azure-core, azure-search-documents, qdrant-client, mem0ai\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2025.1\n",
            "    Uninstalling pytz-2025.1:\n",
            "      Successfully uninstalled pytz-2025.1\n",
            "Successfully installed azure-common-1.1.28 azure-core-1.32.0 azure-search-documents-11.5.2 backoff-2.2.1 grpcio-tools-1.71.0 isodate-0.7.2 mem0ai-0.1.77 monotonic-1.6 portalocker-2.10.1 posthog-3.23.0 psycopg2-binary-2.9.10 pytz-2024.2 qdrant-client-1.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from mem0 import Memory\n",
        "\n",
        "openai_client = OpenAI()\n",
        "memory = Memory()\n",
        "\n",
        "def chat_with_memories(message: str, user_id: str = \"default_user\") -> str:\n",
        "    # Retrieve relevant memories\n",
        "    relevant_memories = memory.search(query=message, user_id=user_id, limit=3)\n",
        "    memories_str = \"\\n\".join(f\"- {entry['memory']}\" for entry in relevant_memories[\"results\"])\n",
        "\n",
        "    # Generate Assistant response\n",
        "    system_prompt = f\"You are a helpful AI. Answer the question based on query and memories.\\nUser Memories:\\n{memories_str}\"\n",
        "    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": message}]\n",
        "    # Completion API\n",
        "    response = openai_client.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
        "    assistant_response = response.choices[0].message.content\n",
        "\n",
        "    # Create new memories from the conversation\n",
        "    messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "    memory.add(messages, user_id=user_id)\n",
        "\n",
        "    return assistant_response\n",
        "\n",
        "def main():\n",
        "    print(\"Chat with AI (type 'exit' to quit)\")\n",
        "    while True:\n",
        "        user_input = input(\"You: \").strip()\n",
        "        if user_input.lower() == 'exit':\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        print(f\"AI: {chat_with_memories(user_input)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nf5KVXmzwBAI",
        "outputId": "a3b1001b-64f5-4573-833e-0be74791f6be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chat with AI (type 'exit' to quit)\n",
            "You: hola me llamo Matias y tengo 39 años\n",
            "AI: Hola Matías, ¡es un placer conocerte! ¿En qué puedo ayudarte hoy?\n",
            "You: uh perdon creo que te dije mal mi edad\n",
            "AI: No hay problema, Matías. ¿Cuál es tu edad correcta?\n",
            "You: que edad te dije?\n",
            "AI: Me dijiste que tienes 39 años.\n",
            "You: tengo 40\n",
            "AI: ¡Feliz cumpleaños, Matías! Ahora tienes 40 años. ¿Hay algo especial que te gustaría hacer para celebrar este nuevo año?\n",
            "You: exit\n",
            "Goodbye!\n"
          ]
        }
      ]
    }
  ]
}