**Objetivo de esta Etapa:** Establecer el marco mental clave del taller: c√≥mo la complejidad de la tarea (estimada por pasos CoT) dicta la elecci√≥n del tipo de modelo LLM y, crucialmente, el estilo de prompting a utilizar.

---

## Material para Checklist III: Etapa 2 - Framework: Complejidad -> Modelo -> Estrategia (35 min)

### 1. Contenido Clave a Cubrir:
*   Cynefin ultra-simplificado -> Pasos CoT estimados como m√©trica pr√°ctica.
*   **Eje Central:** CoT -> Modelo (`o1` vs `GPT-4o`) -> Estilo Prompt (Minimalista vs Detallado).
*   Ejemplos de tareas de desarrollo mapeadas a este eje.
*   Justificaci√≥n de la adaptaci√≥n del prompt (hallazgos `reasoning-prompt.md`).
*   Por qu√© preferir JSON para salidas estructuradas.
*   Actividad interactiva para aplicar el framework.

### 2. Materiales a Preparar:

*   **Presentaci√≥n de Slides:**
    *   **Slide 7: T√≠tulo de la Secci√≥n**
        *   **T√≠tulo:** El Framework Clave: Complejidad ‚ûî Modelo ‚ûî Estrategia
        *   **Subt√≠tulo:** Eligiendo la herramienta y el enfoque correctos para cada tarea de desarrollo.

    *   **Slide 8: De Cynefin a Pasos CoT (Chain-of-Thought)**
        *   **T√≠tulo:** Midiendo la Complejidad en la Pr√°ctica
        *   **Diagrama Simple:** Cynefin (4 cuadrantes: Simple, Complicado, Complejo, Ca√≥tico) -> Flecha -> "Estimaci√≥n de Pasos CoT".
        *   **Texto Clave:**
            *   "Cynefin nos ayuda a entender la *naturaleza* del problema."
            *   "Para elegir LLM/Prompt, usaremos una m√©trica pr√°ctica: **¬øCu√°ntos pasos de 'pensamiento' (CoT) requiere la tarea?**"
            *   "**Estimaci√≥n R√°pida:**"
                *   **< 3 Pasos (Simple/Claro):** Tareas directas, poca ambig√ºedad. *Ej: Generar boilerplate, traducir snippet, formato simple.*
                *   **3-5 Pasos (Complicado):** Requiere an√°lisis, seguir reglas, descomponer. *Ej: Refactorizar funci√≥n con reglas, explicar c√≥digo complejo, resumir con formato espec√≠fico.*
                *   **> 5 Pasos (Complejo):** Requiere creatividad, planificaci√≥n, m√∫ltiples dependencias, incertidumbre. *Ej: Dise√±ar arquitectura, planear migraci√≥n, resolver bug profundo, generar estrategia.*
                *   **(Ca√≥tico):** Respuesta inmediata bajo presi√≥n. *Ej: Debugging cr√≠tico en producci√≥n.*

    *   **Slide 9: El Eje Central: CoT ‚ûî Modelo ‚ûî Estilo de Prompt**
        *   **T√≠tulo:** La Decisi√≥n Clave: Adaptando tu Enfoque
        *   **Diagrama Visual (Eje o Tabla):**
            | Pasos CoT Estimados | Dominio (Aprox) | Modelo Ideal                | Estilo de Prompt Recomendado (Ref: `reasoning-prompt.md`)                                      | Nivel Prompt (Aprox) |
            | ------------------- | --------------- | --------------------------- | ---------------------------------------------------------------------------------------------- | -------------------- |
            | **< 3**             | Simple/Claro    | `GPT-4o`, `Claude Sonnet`   | **Detallado (si es necesario), Claro, Directo.** Funciona bien con ejemplos (Few-Shot).        | Nivel 2-3            |
            | **3-5**             | Complicado      | `GPT-4o` / `o1` (Tradeoff)  | **`GPT-4o`:** Detallado, Nivel 3-4 (con ejemplos, JSON). **`o1`:** M√°s minimalista, Nivel 2-3. | Nivel 3-4 / 2-3      |
            | **> 5**             | Complejo        | `o1`, `Claude Opus` (ideal) | **Minimalista, Zero-Shot.** Evitar CoT expl√≠cito y Few-Shot (>1-2). *Instruir "piensa m√°s"*.   | Nivel 2(-ish)        |
            | **Ca√≥tico**         | Ca√≥tico         | R√°pido (`GPT-4o-mini`?)     | **Directivo, Conciso.** Foco en acci√≥n inmediata.                                              | Nivel 1-2            |
        *   **Mensajes Clave (Debajo o al lado):**
            *   "**¬°Adapta tu prompt!** No es lo mismo pedirle a `GPT-4o` que a `o1`."
            *   "**`o1` (Razonamiento):** Dale el *qu√©*, no el *c√≥mo* detallado. Demasiada gu√≠a lo confunde."
            *   "**`GPT-4o` (No-Raz√≥n.):** Se beneficia de instrucciones claras, ejemplos y estructura (JSON)."

    *   **Slide 10: ¬øPor Qu√© Adaptar? Insights Clave**
        *   **T√≠tulo:** Evidencia de la Investigaci√≥n (`reasoning-prompt.md`)
        *   **Puntos Clave (iconos + texto corto):**
            *   üìâ **Few-Shot Degrada `o1`:** Demasiados ejemplos empeoran resultados en modelos de razonamiento.
            *   üìâ **CoT Expl√≠cito Reduce Perf. `o1` (Tareas Simples):** Lo hacen internamente; guiarlos puede confundir.
            *   üëç **`o1` Mejora con "Tiempo de Pensamiento":** Instruir a razonar m√°s (sin dar pasos) s√≠ ayuda en tareas complejas.
            *   üëç **`GPT-4o` Mejora con Gu√≠a:** Prompts detallados, CoT, Few-Shot siguen siendo √∫tiles.
            *   ‚ö†Ô∏è **Formato Salida:** `o1` puede ser menos fiable con formatos estrictos que `GPT-4o`.

    *   **Slide 11: La Importancia del Output Estructurado (JSON)**
        *   **T√≠tulo:** JSON: Tu Aliado para la Robustez
        *   **Imagen:** Snippet de c√≥digo JSON.
        *   **Puntos Clave:**
            *   **Fiabilidad:** Fuerza al LLM a seguir una estructura. Crucial si `o1` tiene problemas de formato.
            *   **Integraci√≥n:** F√°cil de parsear y usar en tu c√≥digo (`backend`, `frontend`).
            *   **Claridad:** Define expl√≠citamente qu√© datos esperas.
            *   **Recomendaci√≥n:** Usar formato `json_schema` con `strict: True` (API OpenAI) siempre que necesites datos consistentes.

    *   **Slide 12: Actividad Interactiva - ¬°A Clasificar!**
        *   **T√≠tulo:** Tu Turno: Aplica el Framework
        *   **Instrucci√≥n:** "Abre el board de Miro/Figjam [LINK]. Lee los 3 escenarios de desarrollo. Arrastra cada escenario a la columna que mejor represente su **CoT Estimado**, **Modelo Ideal** y **Estilo de Prompt**."
        *   **(Espacio para embeber/mostrar el board interactivo)**

*   **Board Interactivo (Miro / Figjam):**
    *   **T√≠tulo:** Framework CoT -> Modelo -> Prompt
    *   **Columnas/Zonas Claras:**
        1.  `< 3 CoT` | `GPT-4o` | Prompt Detallado
        2.  `3-5 CoT` | `GPT-4o`/`o1` | Prompt Flexible/Adaptado
        3.  `> 5 CoT` | `o1` | Prompt Minimalista
        4.  `Ca√≥tico` | R√°pido | Prompt Directivo
    *   **Sticky Notes/Tarjetas (3) con Escenarios Dev:**
        *   **Escenario A:** "Escribir una funci√≥n Python simple que valide un formato de email usando regex est√°ndar."
        *   **Escenario B:** "Planificar la migraci√≥n completa de un m√≥dulo legacy PHP (con 5 clases interdependientes y conexi√≥n a BD) a Node.js, identificando riesgos y pasos principales."
        *   **Escenario C:** "Generar 10 variaciones creativas y concisas para el texto de un bot√≥n de 'Call to Action' en una landing page."
    *   **Instrucciones en el Board:** "Arrastra cada escenario (A, B, C) a la columna que mejor describa su complejidad y el enfoque LLM/Prompt ideal."

*   **Script/Guion del Instructor (Puntos Clave):**
    *   **(5 min) Recapitulaci√≥n y Conexi√≥n:** "Recordando el pre-reading, tenemos Cynefin y los 4 niveles de prompt. Ahora, vamos a conectar esto de forma pr√°ctica. La clave es estimar la complejidad. En lugar de debatir si algo es 'Complicado' o 'Complejo', usaremos los **Pasos de Chain-of-Thought (CoT)** como gu√≠a." *(Explicar Slide 8 y los rangos estimados con ejemplos dev)*.
    *   **(10 min) El Eje Central:** "Una vez estimado el CoT, tomamos la decisi√≥n m√°s importante (Slide 9): **¬øQu√© modelo usar y c√≥mo pedirle las cosas?** Aqu√≠ es donde entra la diferencia entre `o1` y `GPT-4o`. Para tareas simples (<3 CoT), `GPT-4o` es eficiente y sigue bien el formato. Para tareas muy complejas (>5 CoT), `o1` es el ideal, pero necesita un **prompt minimalista**. ¬°No le digas c√≥mo pensar paso a paso! Dale el objetivo claro y conciso. En el rango intermedio (3-5 CoT), hay un *trade-off*: `GPT-4o` con un prompt muy bueno puede funcionar, o `o1` con un prompt simple. Tendr√°s que experimentar." *(Enfatizar la tabla/eje)*.
    *   **(5 min) Justificaci√≥n (Insights & JSON):** "¬øPor qu√© esta diferencia? La investigaci√≥n reciente (Slide 10, ref `reasoning-prompt.md`) muestra que guiar demasiado a `o1` empeora los resultados. ¬°Es contra-intuitivo! Por otro lado, `GPT-4o` agradece la gu√≠a. Y si necesitamos s√≠ o s√≠ un formato espec√≠fico, especialmente si `o1` tiende a desviarse, **JSON es la soluci√≥n** (Slide 11). Nos da estructura y fiabilidad para integrar la salida en nuestro c√≥digo."
    *   **(5 min) Actividad Interactiva (Instrucciones):** "¬°Ok, vamos a aplicar esto! Abran el link de Miro/Figjam que ven en pantalla/chat (Slide 12). Ver√°n 3 escenarios de desarrollo (A, B, C) y 4 columnas representando nuestro framework. T√≥mense 3-4 minutos para leer los escenarios y arrastrar cada uno a la columna que crean correcta seg√∫n su CoT estimado, modelo ideal y estilo de prompt." *(Iniciar actividad, observar)*.
    *   **(10 min) Revisi√≥n y Discusi√≥n:** "¬°Tiempo! Veamos los resultados... Interesante c√≥mo clasificaron el Escenario B (Migraci√≥n PHP). La mayor√≠a lo puso en '>5 CoT / o1 / Minimalista', ¬°lo cual tiene sentido por la complejidad e incertidumbre! ¬øAlguien que lo haya puesto diferente quiere comentar por qu√©?" *(Facilitar discusi√≥n sobre 1-2 escenarios, clarificar dudas, reforzar el framework)*. "Exacto, la clave es **estimar CoT ‚ûî elegir modelo ‚ûî adaptar prompt**." *(Transici√≥n a la siguiente secci√≥n)* "Ahora que tenemos el mapa, vamos a ver c√≥mo construir estas piezas: prompts efectivos, JSON, y workflows simples..."

### 3. Notas Adicionales para el Instructor:
    *   **Prueba la Herramienta Interactiva:** Aseg√∫rate de que Miro/Figjam est√© bien configurado y sea f√°cil de usar para los asistentes. Ten un plan B si la herramienta falla.
    *   **Claridad del Eje:** El Slide 9 es crucial. Aseg√∫rate de que sea visualmente muy claro y dedica tiempo a explicarlo bien.
    *   **Ejemplos Relevantes:** Aseg√∫rate de que los ejemplos de tareas dev resuenen con la audiencia (JS/PHP/Python).
    *   **Ritmo:** Mant√©n un buen ritmo. Los 35 minutos son ajustados. La actividad interactiva debe ser r√°pida.
    *   **Confianza:** Transmite confianza en el framework, pero reconoce que la estimaci√≥n de CoT es una habilidad que se desarrolla con la pr√°ctica.

Este material para la Etapa 2 se enfoca en construir el andamiaje conceptual necesario para el resto del taller, conectando teor√≠a (Cynefin, tipos de modelo) con pr√°ctica (estimaci√≥n CoT, estilo de prompt) y valid√°ndolo con una actividad interactiva.